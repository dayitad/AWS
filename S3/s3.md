## S3

Amazon S3 powers the biggest websites in the world and it is one of the most important building blocks of AWS. It is advertised as an infinitely scaling storage so we don't need to provision its size in advance,
and it can grow infinitely. It's extremely popular and we can deploy websites on Amazon S3. Many AWS resources have an integration with Amazon S3 as well.

#### Buckets and objects

+ S3 is a system, a service that allows us to store objects, so files, into buckets, or directories,
+ Each bucket must have a globally unique name. We can't create a bucket that has already been taken in terms of names. The buckets are defined at the region level. So even though S3 is a global service, buckets are a 
  regional resource,
+ There is a naming convention for buckets which include no uppercase, no underscore, three to 63 characters long. It should not be an IP, and it must start with a lowercase letter or a number.
+ In these S3 buckets we need to create objects, and objects are files and they must have a key.
  + A key is the full path to that file. 
  	+ So if we have a bucket named my-bucket, and a object named my_file.txt, then the key is my_file.txt in s3://my-bucket/my_file.txt.
  	+ But if we have folder structures within our S3 bucket like my_folder1/another_folder/my_file.txt, the key is my_folder1/another_folder/my_file.txt in s3://my-bucket/my_folder1/another_folder/my_file.txt
  + The key can be decomposed in two things, the key prefix and the object name.
  	+ The prefix is my_folder1/another_folder/ and the object name is my_file.txt.
+ There's no concept of directories within buckets, just very, very long key names, but the UI will try to trick you into thinking otherwise because we could "create" directories within S3. But what we really have in S3 is just keys
  with very long names that contain slashes.
+ The object values are the content of the body.
  + A maximum object size on Amazon S3 is 5 TB, so 5,000 gigabytes, which is a huge object but we cannot upload more than 5 GB at a time. 
    That means that if you want to upload a big object of five terabytes, you must divide that object into parts of less than 5 GB, and upload these parts independently into what's called a multi-part upload.
+ Each object in Amazon S3 can have metadata which are key value pairs that could be system or user metadata, and this is to add information onto your objects.
+ Tags - Unicode key value pairs up to 10 which is very useful when you want to have security on your objects, or lifecycle policies.
+ Finally, we'll see when we go into versioning that there is a version ID onto our Amazon S3 bucket.

#### Handson

Go to Amazon S3 console. S3 does not require region selection which makes S3 a global service. But all our buckets are created for a specific region. When we create a bucket, we will still have to select a region.
This Global view will give us all the buckets across all the regions, but a bucket is tied to a specific region. So let us create a bucket. We enter a bucket name and the name must be globally unique. If the bucket name
exists in someone else's account, it's already taken. So lets select a unique bucket name. Then we have to select a region but we should select a region that's close to you as well to create your S3 buckets, and as you can see
not all the regions have S3 just yet. This bucket is going to be tied to this region, even though it will show up as a global service. In bucket settings, there is Block Public Access to prevent your bucket from being public 
by mistake and keep our bucket as private to prevent data leaks. The we click on Create bucket. The bucket has been created, and now I can Go to the bucket details, either by clicking the link, or by clicking the link in the 
console as well. Now we can upload a file. Upload > Add file. Then we get options like information about the permissions of who can see that file, properties on how we want to store that object or Storage class, Encryption, metadata
and tags. > Click on Upload. Now my file is being uploaded onto my S3 bucket. I can click on it, and there's a panel on the right hand side that comes up, and it gives us some information about our file. I'm going to right click on 
this file, and then click on Open. So through this way, I am able to view my file. If I click on the Object URL, I get access denied. So using this public URL for my file in S3, I get a 403 access denied because this file is not public.
But if we use the right click and Open it actually creates a special URL, and this is a very special URL that's very, very long, and this URL is called the pre-signed URL. It's signed with my credentials on AWS, and so because I have the power to view this file, it's going to be included in that URL and I'm going to be able to access this file. We can create a folder called images, and click on Save, and then within this folder, I can go ahead again to upload my file.
If we look at our S3 bucket, we have two keys. We have one file at the root of our S3 bucket and another within the key images. This gives us the illusion of a directory. We can perform operations like rename the file, or delete the file, 
Copy, Move etc. 

#### Versioning

Our files in Amazon S3 can be versioned but it has to be enabled first at the bucket level. That means that if you re-upload a file version with the same key, it will not be overridden. It will create a new version of that file
like version one, then version two, then version three and so on. It is best practice to version your buckets in Amazon S3 in order to be able to have all the file versions for a while because we can get protected against 
unintended deletes, and are able to restore a previous version, and also you can easily rollback to any previous versions you wanted.
+ Any file that is not versioned prior to enabling versioning will have the version, Null
+ If you suspend versioning in your bucket, it does not delete the previous versions, it will just make sure that the future files do not have a version assigned to it.

#### Versioning Handson

Bucket > Properties > Versioning > Enable Versioning > Save
+ This is to keep multiple versions of an object in the same bucket. Back in Overview, now we have a new panel called Versions and we can hide or show it. If we click show, we have a different UI into Amazon S3 console
  which is showing the version ID on top of the file name. 
+ So for a file uploaded before versioning enabled, the version ID is Null. 
+ Now we add a new file and see that it has a version ID which is a complicated string. If we reupload the file, we see now we have 2 versions of the file each with a different version id. Even though the files were same they were 
  not overwritten. But if we click hide on the versioning tab, we just see 2 files, not the multiple versions. 
+ Now we delete a file and my file is gone. But if we click on show in versioning, we see that a delete marker has been added for the file. A delete marker is a file with zero size that has a version ID and the delete marker 
  is hiding my file since it is deleted. But what I can always restore a previous version. I can delete the delete marker and the latest version is one of the previous versions and my file is back. This is how we can prevent
  against unintended deletes.
+ If we go ahead and delete a specific version of a file, then that version is deleted permanently. 
+ We can suspend the versioning. Any previous objects are still here and still have a version ID. It's just affecting future version objects.

#### Encryption

When we upload objects onto Amazon S3, we upload the objects into servers of AWS. We want to ensure that these objects are not accessible and follow security standards set up by your company.
So Amazon gives you four methods to encrypt objects in Amazon S3.

+ SSE-S3 - This is to encrypt S3 objects using keys handled and managed by AWS.
+ SSE-KMS - This is to leverage AWS key management service to manage your encryption keys.
+ SSE-C - When you manage your own encryption keys 
+ Client Side Encryption

###### SSE-S3

This is an encryption, where the keys used to encrypt the data are handled and managed by Amazon S3. The object is going to be encrypted server side. SSE means Server Side Encryption. And the type of encryption is AES-256,
which is an algorithm. To upload an object and set the SSE-S3 encryption, you must set a header called x-amz-server-side-encryption to AES256. x-amz stands for x Amazon. Amazon S3, thanks to this header, knows that it should apply
its own S3 managed data key. And using the S3 managed data key some encryption will happen and the object will be stored encrypted into your Amazon S3 buckets. The data key is entirely owned and managed by Amazon S3.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SSE_S3.png" width="60%" height="60%"/>

###### SSE-KMS

KMS is Key Management Service, which is an encryption service for you. So SSE-KMS is when you have your encryption keys that are handled and managed by the KMS service. This gives you control over who has
access to what keys and also gives you an audit trail. Each object is going to be again encrypted server side. And for this to work, we must set the header x-amz-server-side-encryption to a value of AWS:KMS.
This is also server side encryption. We use the header to let Amazon S3 know to apply the KMS customer master key you have defined on top of the object. And using this customer master key our objects 
will be encrypted server side and stored in our S3 buckets under the SSE-KMS encryption scheme.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SSE_KMS.png" width="60%" height="60%"/>

###### SSE-C

This stands for server-side encryption. Here we use keys that you provide yourself outside of AWS. Amazon S3 does not store the encryption key you provide. It will use it for encryption and then that key
will be discarded. For this, To transmit the data into AWS, you must use HTTPS, because you're going to send a secret to AWS. And so you must have encryption in transit. Encryption key must be provided
in the HTTP headers for every HTTP request made because it's going to be discarded every single time. So we have the object and we want to have it encrypted in Amazon S3 with our own Client Side data key.
So we send both these things over HTTPS. It's an encrypted connection between the clients and Amazon S3, and the data key is in the header. It is again server side encryption.Amazon S3 will perform the 
encryption and store the encrypted object into your S3 buckets. If you wanted to retrieve that file from Amazon S3 using SSE-C, you would need to provide the same Client Side data key that was used.
So it requires a lot more management on your end, because you manage the the data keys. And Amazon or AWS in general does not know which data keys you have used.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SSE_C.png" width="60%" height="60%"/> 

###### Client Side Encryption.

This is when the client encrypts the objects before uploading it into Amazon S3. Some client libraries like Amazon S3 Encryption Client is a way to perform that Client Side Encryption.
Clients must encrypt data before sending it to S3. And then in case you receive data that is encrypted using Client Side Encryption. Then you are solely responsible for decrypting the data yourself as well.
So you need to make sure you have the right key available. Here the customer manages entirely the keys and the encryption cycle. Amazon S3, this time is just a bucket where it's not doing any encryption for us because it is Client Side Encryption and not Server Side Encryption. The client will use a encryption SDK, like the S3 encryption SDK and the encryption will happen client Side. We are going to just 
upload that already encrypted object into Amazon S3.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/CUSTOMER.png" width="60%" height="60%"/> 

#### Important points

Encryption in transit is around SSL and TLS connections. Amazon S3 is an HTTP service. And it exposes HTTP endpoint that is not encrypted, and it exposes an HTTPS endpoint, which is encrypted and provide what's called encryption in flight. It relies on SSL and TLS certificates. So you're free to use the endpoints you want. But if you use the console, for example, you would be using HTTPS.
Most clients would, use HTTPS endpoints by default. If you're using HTTPS, that means that the data transfer between your clients and Amazon S3 is going to be fully encrypted. And that's what's called encryption in transit.

In case you're using SSE-C, so server side encryption, and the key is provided by your clients, then HTTPS is mandatory. 

#### Encryption handson

If I click on my file right now, and we go on the right hand side, right now it says encryption none. So my file is not encrypted. 
So we upload a file and click Next. On the encryption side, there is two or there are three options. There's None, which is no encryption. And there is Amazon S3 master key(SSE-S3) and Amazon AWS-KMS master key(SSE-KMS). If we choose SSE-S3, We don't have to provide any key. But if we use SSE-KMS, then we have to select a KMS key. And by default, there is going to be a service CMK. So AWS-S3 which is a key
dedicated to Amazon S3 that we could use. Or we could provide our own custom KMS key, in which case, we would have to create that key in advance. We will use Amazon S3 Master key, click on Next. And finally upload. Our file has been uploaded, and on the right hand side, it says encryption AES-256. This has been encrypted. If I go to versions show, we can see the latest version of our file is encrypted, but the version prior to that is not encrypted. So the encryption really belongs to the object you have uploaded. Now I will choose KMS and choose AWS/S3 and click on Upload. and this time the encryption scheme is AWS-KMS. We cannot do SSE-C, through the AWS console, we can do it using the command line interface.

#### Default Encryption

If I go to properties, We have Default Encryption, to automatically encrypt objects stored in Amazon S3. This means that if you are uploading an object, and you don't specify an encryption scheme,
we get three options. Either we leave it unencrypted, or we by default, apply AES-256 or AWS-KMS. So for example, we can say I want to automatically encrypt all my objects with SSE S3 and click on Save.
And now by default if we upload an object without setting any encryption, we can see the encryption is AES-256. 

#### S3 Security and Bucket Policies

+ First we have user-based security. So our IAM users have IAM policies, and they authorize which API calls should be allowed and if our user is authorized through IAM policy how to access our Amazon S3   
  bucket, then it's going to be able to access it.
+ Then we have resource-based security 
  + S3 bucket policies - They're bucket-wide rules that we can set in the S3 console and what they do is that they will say what principals can and cannot do on our S3 bucket. And this enables us to do 
    cross account access to our S3 buckets.
  + Object ACL which is finer grained where we set at the object level the access rule.
  + Bucket ACL, even less common
+ An IAM principal, so it can be a user, a role, can access an S3 object if the IAM permissions allow it, so that means that you have an IAM policy attached to that principal that allows access
  to your S3 bucket, or if the resource policy, usually your S3 bucket policy, allows it.
+ And you need to make sure there is no explicit deny. So if your user through IAM is allowed to access to your S3 bucket but your bucket policy is explicitly denying your user to access it, then you will 
  not be able to access it.

###### S3 bucket policies.

They're JSON-based policies. In the following example, the JSON bucket policy allows public read on our S3 buckets. It says effect allow, principal is ```*```, so anyone, can perform the the action GetObject,
on the resource, ```examplebucket/*``` which means on any objects within my S3 bucket. So this allows public access to our S3 buckets. These bucket policies can be applied to your buckets and objects, so both. The actions is they allow a set of API to allow or deny. The effect is allow or deny, the principal is the account or the user that this S3 bucket policy applies to.

Some common use cases for S3 bucket policies is to grant public access to a bucket or to force objects to be encrypted at the upload time, or to grant access to another account using cross account S3 bucket policies.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/S3_BUCKET_POLICY.png" width="60%" height="60%"/> 

#### BLOCK PUBLIC ACCESS

We also have the bucket settings for block public access. So this was a new setting that was created to block objects from being public if the account had some restrictions.
We have four different kinds of block public access settings.
+ new access control lists
+ any access control lists
+ new public or access point policies. So this is going to block 
  objects and buckets from becoming public if they're granted through any of these methods
+ you can block public and cross account access to buckets and objects 
  through any public bucket or access point policy.
These settings historically were created to prevent company data leaks because there were a lot of leaks of Amazon S3 bucket in the news and Amazon S3 came up with this way of making sure that any server 
could say, hey, none of my buckets are public, by the way, because of the settings, and that was very popular. If you know that your buckets should never, ever be public, leave these on. And there's a way to set these at the account level. 

#### Other Security features of S3
+ On the networking side, you can access S3 privately through VPC endpoints. So if you have EC2 instances in your VPC without internet access, then they can access S3 privately through what's called a VPC
  endpoint.
+ For logging audit, you can use S3 access logs and they can be stored in the other S3 buckets.
+ API calls can also be logged into CloudTrail, which is a service to log API calls in your accounts.
+ For user security, you have MFA Delete, so multifactor authentication is MFA. In which case if you want to delete a specific version of objects in your buckets, then you can enable MFA Delete 
  and we will need to be authenticated with MFA to be able to delete the objects.
+ Pre-signed URLs which was a very, very long URL signed with some credentials from AWS and it's valid only for a limited time. The use case for it, is to download a premium video from a service if the user 
  is logged in and has purchased that video. If you see the access of certain files to certain users for a limited amount of time, think pre-signed URLs.

#### Bucket policies handson

+ S3 bucket policy - Go to "Permissions" - "Bucket Policy." Here we are able to set a bucket policy for our S3 buckets. We can reference Documentation to get some sample policies or use the Policy Generator.
  We're going to design an Amazon S3 bucket policy, that will prevent us from uploading an object unencrypted without AES-256 encryption. So select policy type as "S3 Bucket Policy," and then the "Effect" is going to be "Deny."
  We will deny principal star, which means anyone. For the "Actions," we're talking about upload, so the action is called "Put Objects." So, we deny anyone on Amazon S3, for the action, "Put Objects," and then we have to give 
  the Amazon Resource Name, or ARN. This policy will be applied to anything within the buckets. So, at the end of the ARN, you need to add a ```/*``` which represents any object within the buckets.Then we add the statements.
  Then, we need to add conditions. The first condition is ensuring a header for encryption is not null.The condition is null and key is going to be the encryption header name and value is true. The  second condition is that the header 
  value is AES256 so condition is StringNotEquals and Key is s3:x-amz-server-side-encryption and value is AES256. Thwn we generate a policy. And now if we try to upload a non encypted file it will fail(forbidden). KMSencryption will also
  not work here. 

  ```
  {
	    "Version": "2012-10-17",
	    "Id": "DenyUploadWithoutSSES3Encryption",
	    "Statement": [
		  {
			    "Sid": "Stmt1605720169066",
			    "Effect": "Deny",
			    "Principal": "*",
			    "Action": "s3:PutObject",
			    "Resource": "arn:aws:s3:::dhrubas-bucket/*",
			    "Condition": {
			        "Null": {
			            "s3:x-amz-server-side-encryption": "true"
			        },
			        "StringNotEquals": {
			            "s3:x-amz-server-side-encryption": "AES256"
			        }
			    }
			}
		]
	}
  ```

+ We also have access control lists here where we can set "Access Control List" at the bucket level or on this object under permissions.

+ If we go to "Permissions," - "Block public access," there are four settings which are blocking all public access. To make your bucket public, we need to un-tick everything and click on "Save," These settings can also be applied at the 
  account level. So under account settings we have to enable "Block public access" when we know that all the S3 buckets in your account should not be public. This will provide an extra security.

#### S3 Websites

S3 can host static websites and have them accessible on the worldwide web and the website URLs will be very simple, it will be an HTTP endpoint.
It will look like this.
+ <bucket-name>.s3-website-<AWS-region>.amazonaws.com or <bucket-name>.s3-website.<AWS-region>.amazonaws.com depending on the region you're in.
It starts with a bucket name.s3-website and then the AWS region.amazonaws.com. 
If you enable it for a website but you don't set a bucket policy that allows public access to your bucket, you will get a 403 forbidden error.

We have created index.html and error.html and uploaded them to our bucket. Now we will go to Properties and then Static website hosting and I will say yes, please use this bucket to host a website.
The index document is the document that you get by default which in this case is index.html and the error document is the document that will be showing in case I have the wrong URL, which is error.html.
For redirection rules, we'll just leave it empty and click on Save. After that we will get an endpoint which is the URL of the website hosted on S3. If we click on it we will get a 403 forbidden access denied.
This is because the S3 bucket is a private S3 bucket. So we need to make sure that this S3 bucket becomes public so that we can access our files in here. So we have to do 2 things. 
+ We need to change these block public access settings to make sure that we do enable this bucket as a public bucket.
+ We need to create a bucket policy and we will allow anyone on Amazon S3 to view the website(do a get object). We will give the ARN name/* We generate the policy and click on save. We get a message saying 
  "This bucket has public access,"
  ```
  {
	  "Id": "Policy1605717565814",
	  "Version": "2012-10-17",
	  "Statement": [
	    {
	      "Sid": "Stmt1605717561605",
	      "Action": [
	        "s3:GetObject"
	      ],
	      "Effect": "Allow",
	      "Resource": "arn:aws:s3:::dayita-bucket/*",
	      "Principal": "*"
	    }
	  ]
	}
  ```
Now if we use a wrong URL, it will redirect us to error.html.

```
index.html

<html>
	<head>
		<title>My First Webpage</title>
	</head>
	<body>
		<h1> I love coffee </h1>
		<p> Hello World </>
	</body>

	<img src="coffee.jpg" width=500/>	
</html>

error.html
<h1>Uh oh, there was an error</h1>
```

#### S3 Cors

CORS is Cross-Origin Resource Sharing.
+ Origin - An origin is a scheme, so a protocol, a host, a domain, and a port. In English, this means that if we go to https://www.example.com, this is an origin where the scheme is HTTPS, the host is www.example.com, 
  and the port is port 443.
Cross-Origin Resource Sharing means that we want to get resources from a different origin. The Web Browser has CORS security in place which means as soon as you visit a website, you can make request to other origins 
only if the other origins allow you to make these request. This is a browser-based security. So what is the same origin and what is a different origin? 
+ Same origin is where you go on example.com/app1 or example.com/app2. This is the same origin, so we can make requests from the web browser from the first URL to the second URL because this is the same origin.
+ But if you visit, for example, www.example.com and then you're asking your web browser to make a request to other.example.com, this is what's called a cross-origin request and your web browser will block it
  unless you have the correct CORS headers. The request will not be fulfilled unless the other origin allows for the request using the CORS Headers. The CORS Headers is called Access-Control-Allow-Origin.

Our web browser visits the first web server, and because this is the first visit we do, it's called the origin. So for example, our web server is at https://www.example.com. There is a second web server called a 
cross-origin because it has a different url, which is https://www.other.com. So the web browser visits our first origin and it's going to be asked from the files(index.html) that are uploaded at the origin to make a 
request to the cross-origin. The web browser will do a preflight request which will ask the cross-origin if it is allowed to do a request on it. The origin reponds with headers like Access-Control-Allow-Origin which states if
this website is allowed or not. If it returns methods that are authorized as GET, PUT, and DELETE. we can get a file, delete a file, or update the file.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/CORS.png" width="60%" height="60%"/> 

In S3 CORS, if a client does a cross-origin request, on our S3 bucket enabled as a website, then we need to enable the right CORS headers. We can allow for a specific origin by specifying the entire origin name, or as star * 
for all origins. The web browser for example, is getting HTML files from our bucket enabled as a website. But there is second bucket that is going to be our cross-origin bucket, also enabled as a website, that contains some files 
that we want. So, we're going to do GET index.html and the website will say, okay here is your index.html and that file is going to say you need to perform a GET for another file on the other origin. And if the other bucket is configured
with the right CORS headers, then web browser will be able to make the request, if not it will not be able to make that request. The CORS headers have to be defined on the cross-origin bucket, not the first origin bucket.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/CORS_S3.png" width="60%" height="60%"/> 

```
index.html

<html>
	<head>
		<title>My First Webpage</title>
	</head>
	<body>
		<h1> I love coffee </h1>
		<p> Hello World </>
	</body>

	<img src="coffee.jpg" width=500/>	

	<div id="tofetch"/>
	<script>
		var tofetch = document.getElementById("tofetch");
		fetch('use the cors bucket/extra-page.html').then((response) => {
					return response.text();
				}).then((html) => {
					tofetch.innerHTML = html
				});
	</script>
</html>

extra-page.html

<p>This <strong>extra page</strong> has been successfully loaded!</p>
```

We upload index.html and error.html to the primary bucket and extra-page.html to the cross origin bucket. Then we make both buckets public and enable static page hosting. Then we add CORS config to the CORS bucket and everything works.
```
CORS_CONFIG.xml
<CORSConfiguration>
    <CORSRule>
        <AllowedOrigin>origin bucket url here, we can also put * </AllowedOrigin>
        <AllowedMethod>GET</AllowedMethod>
        <AllowedMethod>POST</AllowedMethod>
        <AllowedMethod>DELETE</AllowedMethod>
        <MaxAgeSeconds>3000</MaxAgeSeconds>
        <AllowedHeader>Authorization</AllowedHeader>
    </CORSRule>   
</CORSConfiguration>

or in new console only JSON

[
    {
        "AllowedHeaders": ["*"],
        "AllowedMethods": ["GET","POST","DELETE"],
        "AllowedOrigins": ["*"],
        "ExposeHeaders": [],
        "MaxAgeSeconds": 3000
    }
]
```
#### S3 Consistency model

Amazon S3 is a new eventually consistent system. It is made up of multiple servers so when you write to Amazon S3, the other servers are going to replicate data between each other. And this is what leads to different consistency issues.
+ You get read after write consistency for PUTS of new objects so that means that as soon as you upload a new object, once you get a correct response from Amazon S3, then you can do a GET of that object and get it. So that means 
  that if you do a successful PUTS so PUT 200, Then you can do a GET and it will be 200. 
+ If you do a GET before doing the PUT to check if the object existed, you may get a 404 for not existing. There is a chance that you would get a 404 still even though the object was already uploaded and this is what's called eventually  
  consistent. So we have eventual consistency, on DELETES and PUTS of existing objects. So if you read an object right after updating it, you may get the older version of that object. If you do a PUT on an existing object so PUT 200, then you do another PUT 200, and then you do a GET, then the GET might return the older version if you are very, very quick. To get the newer version, you have to just to wait a little bit.
+ If you delete an object, you might still be able to retrieve it for a very short time. So if you do delete an object and you do a GET right after , you may have a successful GET, so GET 200. If you retry after a second or five seconds,
  then the GET will give you a 404 because the object has been deleted.
So read after write consistency for PUTS of new objects and eventual consistency for DELETES and PUTS of existing objects. There is no way to request strong consistency in Amazon S3, you only get eventual consistency and there's no API to  get strong consistency. So that means that if you overwrite an object, you need to wait a little bit before you are certain that the GET returns the newest version of your object.

#### MFA Delete

+ Multi Factor Authentication(MFA) - It forces our users to generate a code on a device which could be a mobile phone or your hardware key to do important operations on S3.
+ So, to use MFA-Delete, we have to first enable versioning on the S3 bucket.
+ We will need MFA to
	+ permanently delete an object version
	+ suspend versioning on the bucket.
+ The one important thing to know is that MFA-Delete can be enabled or disabled only by the bucket owner, which is the root account. Even if you have an administrator account, you cannot enable MFA-Delete. 
  You'll have to use the root accounts.
+ We can do MFA-Delete only using the CLI for now.
+ Handson - Create a buket and enable versioning. Then we go to the IAM Management Console in root account and go to MFA and link your device and get the serial number. Then generate access keys and these are root access keys.
  Then we will have to do ```aws configure --profile root-dhruba``` and give access key id and access key secret and region. So if we are doing something like ```aws s3 ls --profile root-dhruba```, I am using root credentials.
  Then run a command like this.
  ```$ aws s3api put-bucket-versioning --bucket my-bucket-name --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa “arn-serial-of-mfa-device  6-digit-mfa-code” --profile root-dhruba```. Now if we try to delete anything
  permanently from the bucket, we cannot delete anything. We have to delete from bucket. We also cannot suspend versioning without CLI.

#### S3 Default Encryption

+ Two or three years ago, the old way to enable default encryption was to use a bucket policy. And you would refuse any HTTP command without the proper headers. Se we would create a bucket policy where we would say that I'm denying any put 
  request as long as there is no header saying that there's going to be server side encryption of AES-256. And then you would add another one saying that I'm going to deny anything that is unencrypted.
+ The new way is we just have to tick default encryption setting in S3. We will get this setting under bucket settings.
+ One thing to note is that bucket policies will be evaluated before the default encryption settings.
+ So if we enable default encryption in s3 and upload a file in s3, the file will be automatically encrypted.

#### S3 logs

+ For audit purposes, you want to log all the access into your S3 buckets. That means that any request that is done to Amazon S3 from any accounts, authorized or denied, you want it to be logged into another S3 bucket
  so you can analyze it later using data analysis tools like Amazon Athena.
+ We make requests into a bucket and that bucket has been enabled for logging into another bucket, a logging bucket. Once we've enabled the S3 access logs, we'll log all the requests into the logging bucket.
+ The log format is https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html
+ Never set the logging bucket to be the monitored bucket. Else, it will ceate a logging loop causing your bucket to grow exponentially. So say we have a bucket. It happens to be our application bucket and also the bucket 
  that is going to receive all the logs. So whenever a user puts an object, the bucket is going to log it inside of itself, create a logging loop and they will create a new object, that will be logged in a new object, 
  that will be logged, and so it creates an infinite logging loop.
+ Create an access log bucket and a bucket to monitor. In this bucket to monitor, go to properties and enable server access logging and enter the target bucket name. Anything that is done to this second bucket
  like upload, delete, encryption, version enabling, everything will be logs to the access logs buckets. It takes some time and after that the logs are present in access logs bucket. We can use Athena to analyze this
  logs.

|S3 Logging|Logging Loop|
|----------|------------|
|<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/S3_LOGGING.png" width="60%" height="60%"/>|<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/LOGGING_LOOP.png" width="60%" height="60%"/>|

#### Amazon S3 Replication

We have CRR and SRR for cross-region replication and same-region replication.
+ We have an S3 bucket for example in one region and we want to replicate it asynchronously into another region, into another bucket.
	+ We must first enable versioning in the source and destination buckets to set up replciation.
	+ Setup Cross-region replication if the two buckets are in different regions or same-region replication, SRR if the two buckets are in the same region.
	+ The buckets can be in different accounts. So it is possible to save a copy of your data into another account using S3 replication.
	+ The copying happens asynchronously, but it is very, very quick. And for the copying to happen, you need to create an IAM role which will have the permissions to copy data 
	  from the first S3 bucket to the second S3 bucket.
	+ So the use cases for cross-region replication is compliance or lower latency access of your data into other regions or to do cross account replication.
	+ Use case for SRR or same region replication can be log aggregation(So if we have different logging buckets and want to centralize them into one bucket) or live replication between a production and test accounts.
	+ After you activate S3 replication, only the new objects are replicated. So it's not retroactive. It will not copy your existing states of your S3 buckets
	+ For a delete operation, nothing is replicated.
		+ So if you delete without a version ID, it will add a delete marker and it's not going to be replicated.
		+ And if delete with version ID, again you will delete at the source and it's not replicated.
	+ And finally there is no chaining of replication. That means that if bucket one has replication to bucket two which has replication into bucket three, then any objects working in bucket one will be in bucket two but will not be 
	  replicated to bucket three so we can not chain our replication.
	+ We create a bucket in one region and a replica bucket in another region. We enable versioning in both buckets. Then we go to management , replciation and add rule to replicate entire bucket and also choose replication bucket.
	  AWS will automatically choose SRR or CRR based on the bucket locations. Then we have to create an IAM role and set status as enabled. This policy attached to this role allows us to get or list on origin bucket and replication action
	  on the target bucket. Then if we upload a file, we can see the file on the replica bucket. If we delete the file in origin bucket, then the file does not get deleted in the replica bucket. So any delete action is not replicated.

#### S3 Presigned URLS

+ You can generate pre-signed URLS using either the SDK or CLI.
+ For downloads, you can just do the CLI but for uploads it's a bit harder, and you must use the SDK to use them.
+ Now, when you generate a pre-signed URL by default, it will have an expiration of 3600 seconds, which is one hour. And you can change that timeout using an --expires-in parameter argument and you specify the [TIME_IN_SECONDS].
+ And when you give a presigned URL to an user, they will inherit the permissions of the person who generated the URL. So they can do GET or PUT accordingly.
+ There's many reasons for a pre signed URL. 
	+ Maybe you want to allow only logged in users to download a premium video on your S3 bucket maybe for 15 minutes only.
	+ Maybe you have an ever changing list of users that need to download files and so you don't want to give them access directly to your bucket because it could be very dangerous and not maintainable, because there's so many 
	  new users all  the time. So maybe you want to generate URLs dynamically and give them the URLs over time by pre-signing all of them.
	+ Maybe you want to allow, temporarily, a user to upload a file to a precise location in our buckets. For example, maybe you want to allow a user to upload a profile picture directly onto our S3 bucket.
+ Generate a presigned URL for download.
	+ In CLI do the following.
	  ```
	  aws configure set default.s3.signature_version s3v4 // This is for compatibility with KMS Encrypted Object
	  aws s3 presign s3://bucket_name/beach.jpeg --expires-in 300 --region eu-west-1 //and the URL will be valid for 5 minutes.
	  ```

#### S3 Storage Tiers + Glacier

+ Amazon S3 Standard which is for general purpose.
	+ We have very high durability. It's called 11 nines, so 99.999999999 % of objects across multiple AZ. This means that if you store 10 million objects with Amazon S3 General Purpose,
	  you can, on average, expect to incur a loss of a single object once every 10,000 years. So we will not lose any object on S3 Standard. 
	+ There's a 99.99 availability percentage over a given year.
	+ It can sustain two concurrent facility failures, so it's really resistant to AZ disasters.
	+ The Use Cases for general purpose S3 Standard is going to be Big Data analytics, mobile and gaming applications, content distribution etc.
+ There are more optimized ones depending on our workload.
	+ S3 Infrequent Access or IA also called S3 IA for unknown access patterns. This one is when your files are gonna be infrequently accessed.
		+ This is suitable for data, as the name indicates, that is less frequently accessed, but requires a rapid access when needed.
		+ So we get the same durability across multiple AZ, 99.999999999 % of objects across multiple AZ.
		+ There's a 99.9 availability percentage over a given year.
		+ It is lower cost compared to Amazon S3 Standard. The idea is that if you access your object less, you won't need to pay as much.
		+ It can sustain two concurrent facility failures.
		+ The Use Cases for this is going to be a data store for disaster recovery, backups, or any files that you expect to access way less frequently.
	+ S3 One Zone-IA, when we can recreate non critical data.
		+ S3 One Zone-IA or Infrequent Access is the same as IA but now the data is stored in a single availability zone.
		+ Before, it was stored in multiple availability zone, which allowed us to make sure the data was still available in case an AZ went down.
		+ We have the same durability within a single AZ, but if that AZ is somewhat destroyed, so imagine an explosion or something like this, then you would lose your data.
		+ You have less availability, so 99.5% availability.
		+ You have still the low latency and high throughput performance you would expect from S3.
		+ It supports SSL for data at transit and encryption at rest 
		+ It's going be lower cost compared to Infrequent Access by about 20%.
		+ The Use Case for One Zone-IA is going to be to store secondary back up copies of on-premise data or storing any type of data we can recreate. So what type of data can we recreate?
		  Well for example, we can recreate thumbnails from an image. We can store the image on S3 general purpose and we can store the thumbnail on S3 One Zone-Infrequent Access. And if we need to 
		  recreate that thumbnail over time we can easily do that from the main image.
	+ S3 Intelligent Tiering which is going to move data between your storage classes intelligently.
		+ It has the same low latency and high throughput as S3 Standard.
		+ There is a small monthly monitoring fee and auto-tiering fee, and what this will do is that it will automatically move objects between the access tiers based on the access patterns. 
		  So it will move objects between S3 general purpose, S3 IA. And so it will choose for you if your object is less frequently accessed or not. And you going to pay a fee for S3 to do that little monitoring.
		+ The durability is the same, it's 11 nines 
		+ It's designed for 99.9 availability over a given year. It can resist an entire event that impacts an Availability Zone, so it's available.
	+ Amazon Glacier for archives
		+ So Glacier is going to be more of an archive. Glacier is cold so think cold archive.
		+ It's a low-cost object storage meant really for archiving and backups
		+ The data needs to be retained long term like tens of years
		+ It's a big alternative to on-premise magnetic tape storage where you would store data on magnetic tapes and put these tapes away. And so if you wanted to retrieve the data
		  from these tapes you would have to find the tape, manually put it somewhere, and then restore the data from it.
		+ So we have still the 11 nines of durability so we don't lose objects.
		+ And the cost per storage is really, really low. $0.004 per gigabyte plus a retrieval cost
		+ Each item in Glacier is not called an object, it's called an archive and each archive can be a file up to 40 terabytes.
		+ Archives are stored not in buckets, they're stored in vaults.
		+ So we have two tiers within Amazon Glacier.
			+ Amazon Glacier - we have three retrieval options
				+ Expedited - retrieval time is one to five minutes, so you request your file and between one to five minutes you will get it back.
				+ Standard - retrieval time is which is three to five hours, so you wait a much longer time.
				+ Bulk, when you require multiple files which takes between five to 12 hours to give you back your files.
			    + Amazon Glacier is really to retrieve files and not have some kind of urgency around it. If you're very, very in a rush you can go and use Expedited, but it's going to be a lot
			      more expensive than using Standard or Bulk.
			    + And the minimum storage duration for Glacier is going to be 90 days. So files that are going to be in Glacier are there for the longer term.
			+ Amazon Glacier Deep Archive. This is for super long term storage, and it's going to be even cheaper.
				+ Standard, 12 hours. So you can not retrieve a file in less than 12 hours.
				+ Bulk, if you have multiple files and you can wait up to 48 hours, it's going to be even cheaper.
				+ So Deep Archive, as we'll see, is going to be for files that you really don't need to retrieve urgently even if it's archived.
				+ The minimum storage duration for Deep Archive is going to be 180 days.
			+ If the storage duration is going to be less than 180 days then you have to use Glacier.
			+ If you need to retrieve your file very, very quickly, between three to five hours, it's going be Glacier.
			+ But if it's going to be a file to be retrieved in 72 hours and it's going to stay for one year in your vault in Glacier, then maybe Deep Archive is going to provide you the best cost savings.
	+ https://aws.amazon.com/s3/storage-classes/
	+ Create a bucket and upload file. Under properties, we can set storage classes and we can even move the storage classes after bucket creation. For glacier, we will be billed for 90 days.

#### Comparison

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/COMPARISON.png" width="60%" height="60%"/>

#### Sample Costing

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/COST.png" width="60%" height="60%"/>

#### Lifecycle Policies

This is to transition objects between storage classes. 
<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/TRANSITION.png" width="60%" height="60%"/>

+ We can go from from STANDARD_IA to INTELIGENT_TIERING, then to ONEZONE_IA, and then to GLACIER or DEEP_ARCHIVE. And it just shows the possible transitions.
+ From GLACIER, you can not go back to STANDARD_IA. You have to restore the objects and then copy that restored copy into IA if you wanted to.
+ So for infrequently accessed objects, move them to 'STANDARD_IA'.
+ For archive objects that you don't need in real-time, the general rule is to move them to GLACIER, or DEEP_ARCHIVE.
+ Moving all these objects around all these classes can be done manually but it can also be done automatically, using something called a lifecycle configuration.
+ S3 Lifecycle Rules
	+ Transition actions which defines when objects are transitioned from one storage class to another. 
		+ For example you're saying, move objects to Standard IA class 60 days after creation. 
		+ And move to Glacier for archiving six month later.
	+ Expiration actions which is to delete an object after some time.
		+ So for example, your access log files maybe you don't need them after another year, so after a year you expire them.
		+ Tt can also be used to delete old versions of a file. So if you have versioning enabled, and you keep on overwriting a file and you know you won't need the previous versions 
		  after maybe 60 days, then you can configure an expiration action to expire objects, old versions of a file after 60 days.
		+ It can also be used to clean up incomplete multi-part uploads in case some parts are hanging around for 30 days and you know they will never be completed. 
		  Then you would set up an expiration action to remove these parts.
    + Rules can be applied for a specific prefix. e.g. s3://mybucket/mp3/* So if you have all mp3 files within the mp3 "folder", or "prefix", 
      then you can set a lifecycle rule just for that specific prefix. So you can have many lifecycle rules based on many prefix, on your bucket.
    + You can also have rules created for certain objects tags so if you want to have a rule that applies just to the objects that are tagged, Department: Finance, then you can do so.

+ Scenario 1: Our application on EC2 creates images thumbnails after profile photos are uploaded to Amazon S3. And these thumbnails can be easily recreated, and you only need to be keep them for 45 days.
  The source images should be able to be immediately retrieved for these 45 days, and afterwards the user can wait up to six hours. How would you design this solution?
  	+ So the S3 source images can be on the STANDARD class, and you can set up a lifecycle configuration to transition them to GLACIER after 45 days because they need to be archived afterwards
  	  and we can wait up to six hours to retrieve them.
  	+ And then for the thumbnails they can be ONEZONE_IA,because we can recreate them, and we can also set up a lifecycle configuration to expire them or delete them after 45 days. In case we lose 
  	  an entire AZ in AWS, we can easily from the source image recreate all the thumbnails.

+ Scenario 2: There is a rule in your company that states that you should be able to recover your deleted S3 objects immediately for 15 days, although this may happen rarely. After this time, and up to one year,
  deleted objects should be recoverable within 48 hours.
  	+ We will need to enable S3 versioning. Because we want to delete files but we want to be able to recover them. And so with S3 versioning we're gonna have object versions and the deleted objects are gonna 
  	  be hidden by "delete marker" and they can be easily recovered. But we're going to have noncurrent versions which are basically the objects or versions from before.
  	+ And so these noncurrent versions we want to transition them into S3_Ia because it's very unlikely that these old object versions are going to be accessed. But if they are accessed, then you need to be sure
  	  to recover them immediately. 
  	+ And then afterwards, after this 15 days of grace period to recover these noncurrent versions, you can transition them into DEEP_ARCHIVE such as for 365 days it can be archived. And they would be recoverable 
  	  within 48 hours. We do not use just GLACIER because GLACIER would cost us a little bit more money, and as we have time till 48 hours, we can use DEEP_ARCHIVE to get even more savings.

+ Bucket - Under Management - Lifecycle - Add Lifecycle Rule - We can add name and filter, storage class transition for current and previous versions, expiration for current and previous versions and click on save.
  We can create multiple rules per bucket for different prefixes.

#### S3 Performance
+ By default Amazon S3 automatically scales to a very very high number of requests and has a very very low latency between 100 and 200 milliseconds to get the first byte out of S3.
+ And, in terms of how many requests per second you can get, you can get 3500 put, copy, post, delete per second per prefix, and 5500 get, head requests per second per prefix in a bucket.
+ And there is no limit to the number of prefixes in a bucket.
+ For example lets have 4 object paths => prefixes.
	+ bucket/folder/subfolder1/file => /folder1/sub1/
	+ bucket/folder/subfolder2/file => /folder1/sub2/
	+ bucket/1/file => /1/
	+ bucket/2/file => /2/
	+ So if we spread reads across all 4 prefixes evenly, we can achieve 22000 requests per second for get and head because each prefix will give us 5500 reads per second.
+ KMS as a limitation to to S3 performance. If you can apply KMS encryption on your objects using SSE-KMS, then you may be impacted by the KMS limits.
	+ When you upload a file, it will call S3 the GenerateDataKey KMS API on your behalf.
	+ When you download a file from S3 using SSE-KMS, it will call the DecryptKMS API.
	+ These two requests will count towards the KMS quota. And so, by default, KMS has a quota of number requests per second and based on the region you're in it could be 5500/second or 10000/second
	  or 30000/second requests and you cannot change that quota. 
	+ As of today, you cannot request a quota increase for KMS. So, what this means is that if you have more than 10000 requests per second in a specific region that only supports 5500 requests per second
	  for KMS, then you will be throttled. So, you need to be sure that KMS doesn't block your performance on S3. Now, this quotas are pretty big for normal usage but still good to know if you have many many files
	  and a high usage of your S3 bucket.
	+ <img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/S3KMS.png" width="60%" height="60%"/>
+ Performance Optimization
	+ Multipart upload 
		+ It is recommended for files > 100 MB and must be used for files greater than 5GB.
		+ It can help parallelize uploads and that will help us speed up the transfers to maximize the bandwidth. It is divided into parts and uploaded parallely to S3 and once all parts are complete,
		  S3 puts them back together into one big file.
		+ <img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/MULTIPART.png" width="60%" height="60%"/>
    + S3 Transfer Acceleration only for upload and not download
    	+ It is to increase transfer speed by transferring a file to an AWS edge location which will forward then the data to the S3 bucket in the target region. 
    	  So, edge locations there are more than regions. There are about two-hundred edge locations today and they are growing.
    	+ Transfer acceleration is compatible with multi-part upload.
    	+ We minimize the amount of public internet that we go through(User to Edge). And, we maximize the amount of private AWS network that we go through(Edge to target S3).
    	+ <img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/TA.png" width="60%" height="60%"/>
    + Reading files efficiently
    	+ S3 Byte-Range Fetches - it can be used to speed up downloads or to retrieve only partial data like headers.
    		+ It is to parallelize GETs by requesting specific byte ranges for your files. 
    		+ In case you fail to get a specific byte range, then you can retry a smaller byte range and thus you have better resilience in case of failures.
    		+ Get partial information very quickly by requesting say first 50 bytes of a file and then the header from that
    		+ <img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/BR.png" width="60%" height="60%"/>

#### S3 Select and Glacier 

+ The the idea is we want to retrieve less data, so subsets of what we're requesting using SQL by performing server side filtering.
+ The SQLs queries are quite simple.They can only be used to filter by rows and columns so they're very simple SQL statements. You cannot do aggregations.
+ You will use less network and less CPU cost client-side because you don't retrieve the full file. S3 will perform the select, the filtering for you and only return to you what you need.
+ So the idea is that before we had Amazon S3 sending all the data into your application and then you have to filter it application-side to find the right rows you want and only keep the columns you want.
  After you request the data from S3 using S3 Select, it only gives you the data you need, the columns you want and the rows you want and the results are up to 400% faster and up to 80% cheaper because 
  you have less network traffic going through and the filtering happens server-side.
+ https://aws.amazon.com/blogs/aws/s3-glacier-select

|CONCEPT DIAGRAM|USE-CASE DIAGRAM|
|---------------|----------------|
|<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SELECTCONCEPT.png" width="60%" height="60%"/>|<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SELECTUSECASE.png" width="60%" height="60%"/>|


#### S3 Event Notifications

+ Some events happen in your S3 bucket like new ObjectCreated, an ObjectRemoved, an object's been restored, or there is some S3 replication happening. We want to be able to react to all these events.
+ You can create rules, and for these rules you can also filter by object names. For example, you want to react to only to the .jpg file. These rules trigger some sort of action inside of your AWS accounts
  like generate thumbnails of images uploaded to Amazon S3.
+ There are 3 possible targets for S3 event notifications
	+ SNS, which is simple notification service, to send notifications in emails.
	+ SQS, for simple queue service, to add messages into a queue.
	+ Lambda functions, to generate some custom code.
+ You can create as many S3 events as desired and most of the time they will be delivered in seconds but sometimes they can take a minute or longer.
+ There's a small caveat, which is that if you want to make sure every single event notification is delivered, you need to enable versioning on your bucket.
	+ If 2 writes are made to a single non-versioned object at the same time , it is possible that only a single notification is sent.
	+ So to ensure that event notification is sent for every successful write, we can enable versioning on our bucket.
+ <img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/EVENT.png" width="60%" height="60%"/>
+ Create Bucket and enable versioning in properties. Then under events, add notification(choose event ad all object creation events and send notification to SQS queue, give SQS queue ARN).
  Queue should be in the same region as bucket. And under permission in the queue, we should allow everyone to perform action send message. And the event has been created. Now if we upload
  a file, 1 message is in the queue which shows a put of the file.

#### Athena

+ It's a serverless service and you can perform analytics directly against S3 files. So, usually you have to load your files from S3 into a database which has redshift and do queries there or something.
  But with Athena, you leave your files in S3 and you do queries directly against them. For this, you can use the SQL language to query the files and it even has a JDBC or a ODBC driver if you wanted 
  to connect your BI tools to it.
+ You get only charged per query and for the amount of data scanned. You just get billed for what you are actually using.
+ It supports many, many, different types of file formats such as CSV, JSON, ORC, Avro, Parquet and in the back end it basically runs Presto which is a query engine.
+ So the use cases for Athena are BI, analytics, reporting, analyze and query VPC Flow Logs, ELB Logs, CloudTrail trails, S3 access Logs, CloudFront Logs etc.
+ How can we analyze data directly on S3 or How can we analyze our ELB Logs How can we analyze our VPC Flow Logs? => Use Athena
+ It is a powerful serverless query engine.

#### Athena Handson

Go to Athena. We can run query on s3 buckets. We have to set up query result location on S3 (which can be a location inside a bucket) before we can run a query. 

```
-- Create a database
create database s3_access_logs_db;
```

```
-- from https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/
-- Create a table
CREATE EXTERNAL TABLE IF NOT EXISTS s3_access_logs_db.mybucket_logs(
         BucketOwner STRING,
         Bucket STRING,
         RequestDateTime STRING,
         RemoteIP STRING,
         Requester STRING,
         RequestID STRING,
         Operation STRING,
         Key STRING,
         RequestURI_operation STRING,
         RequestURI_key STRING,
         RequestURI_httpProtoversion STRING,
         HTTPstatus STRING,
         ErrorCode STRING,
         BytesSent BIGINT,
         ObjectSize BIGINT,
         TotalTime STRING,
         TurnAroundTime STRING,
         Referrer STRING,
         UserAgent STRING,
         VersionId STRING,
         HostId STRING,
         SigV STRING,
         CipherSuite STRING,
         AuthType STRING,
         EndPoint STRING,
         TLSVersion STRING
) 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
         'serialization.format' = '1', 'input.regex' = '([^ ]*) ([^ ]*) \\[(.*?)\\] ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) \\\"([^ ]*) ([^ ]*) (- |[^ ]*)\\\" (-|[0-9]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\"[^\"]*\") ([^ ]*)(?: ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*))?.*$' )
LOCATION 's3://target-bucket-name/prefix/';
```

```
-- We can preview the table which generates a new query like 
SELECT * FROM "s3_access_logs_db"."mybucket_logs" limit 10;
-- The data never left S3 and is analyzed directly in S3
```

We can write queries like below.
```
SELECT requesturi_operation, httpstatus, count(*) FROM "s3_access_logs_db"."mybucket_logs" 
GROUP BY requesturi_operation, httpstatus; -- This will show for operation and status what is the count.

SELECT * FROM "s3_access_logs_db"."mybucket_logs"
where httpstatus='403'; -- We see details of 403 queries
```
#### S3 object Lock and glacier vault lock

+ S3 Object Lock
	+ Adopt WORM model, write once read many which means that you write the file once to your S3 buckets, and then you will block that object version to be deleted for a specified amount of time, 
	  so no one can touch it or modify it. So we have the guarantee that the file will only be written once, and you will not have deletion or modifications happening to that file.
+ Glacier Vault Lock
	+ WORM model
	+ Create a lock policy and that lock policy prevents future edits to that file so that can no longer be changed. Once that policy is set, no one can delete that policy.
	+ Usecases are compliance and data retention requirements - so you want to say okay I want to upload an object to S3, or Glacier, and have the guarantee that no one ever will be able to delete that object,
	  so that we can retrieve in seven years time in case there is an audit.















