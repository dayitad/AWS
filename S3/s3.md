## S3

Amazon S3 powers the biggest websites in the world and it is one of the most important building blocks of AWS. It is advertised as an infinitely scaling storage so we don't need to provision its size in advance,
and it can grow infinitely. It's extremely popular and we can deploy websites on Amazon S3. Many AWS resources have an integration with Amazon S3 as well.

#### Buckets and objects

+ S3 is a system, a service that allows us to store objects, so files, into buckets, or directories,
+ Each bucket must have a globally unique name. We can't create a bucket that has already been taken in terms of names. The buckets are defined at the region level. So even though S3 is a global service, buckets are a 
  regional resource,
+ There is a naming convention for buckets which include no uppercase, no underscore, three to 63 characters long. It should not be an IP, and it must start with a lowercase letter or a number.
+ In these S3 buckets we need to create objects, and objects are files and they must have a key.
  + A key is the full path to that file. 
  	+ So if we have a bucket named my-bucket, and a object named my_file.txt, then the key is my_file.txt in s3://my-bucket/my_file.txt.
  	+ But if we have folder structures within our S3 bucket like my_folder1/another_folder/my_file.txt, the key is my_folder1/another_folder/my_file.txt in s3://my-bucket/my_folder1/another_folder/my_file.txt
  + The key can be decomposed in two things, the key prefix and the object name.
  	+ The prefix is my_folder1/another_folder/ and the object name is my_file.txt.
+ There's no concept of directories within buckets, just very, very long key names, but the UI will try to trick you into thinking otherwise because we could "create" directories within S3. But what we really have in S3 is just keys
  with very long names that contain slashes.
+ The object values are the content of the body.
  + A maximum object size on Amazon S3 is 5 TB, so 5,000 gigabytes, which is a huge object but we cannot upload more than 5 GB at a time. 
    That means that if you want to upload a big object of five terabytes, you must divide that object into parts of less than 5 GB, and upload these parts independently into what's called a multi-part upload.
+ Each object in Amazon S3 can have metadata which are key value pairs that could be system or user metadata, and this is to add information onto your objects.
+ Tags - Unicode key value pairs up to 10 which is very useful when you want to have security on your objects, or lifecycle policies.
+ Finally, we'll see when we go into versioning that there is a version ID onto our Amazon S3 bucket.

#### Handson

Go to Amazon S3 console. S3 does not require region selection which makes S3 a global service. But all our buckets are created for a specific region. When we create a bucket, we will still have to select a region.
This Global view will give us all the buckets across all the regions, but a bucket is tied to a specific region. So let us create a bucket. We enter a bucket name and the name must be globally unique. If the bucket name
exists in someone else's account, it's already taken. So lets select a unique bucket name. Then we have to select a region but we should select a region that's close to you as well to create your S3 buckets, and as you can see
not all the regions have S3 just yet. This bucket is going to be tied to this region, even though it will show up as a global service. In bucket settings, there is Block Public Access to prevent your bucket from being public 
by mistake and keep our bucket as private to prevent data leaks. The we click on Create bucket. The bucket has been created, and now I can Go to the bucket details, either by clicking the link, or by clicking the link in the 
console as well. Now we can upload a file. Upload > Add file. Then we get options like information about the permissions of who can see that file, properties on how we want to store that object or Storage class, Encryption, metadata
and tags. > Click on Upload. Now my file is being uploaded onto my S3 bucket. I can click on it, and there's a panel on the right hand side that comes up, and it gives us some information about our file. I'm going to right click on 
this file, and then click on Open. So through this way, I am able to view my file. If I click on the Object URL, I get access denied. So using this public URL for my file in S3, I get a 403 access denied because this file is not public.
But if we use the right click and Open it actually creates a special URL, and this is a very special URL that's very, very long, and this URL is called the pre-signed URL. It's signed with my credentials on AWS, and so because I have the power to view this file, it's going to be included in that URL and I'm going to be able to access this file. We can create a folder called images, and click on Save, and then within this folder, I can go ahead again to upload my file.
If we look at our S3 bucket, we have two keys. We have one file at the root of our S3 bucket and another within the key images. This gives us the illusion of a directory. We can perform operations like rename the file, or delete the file, 
Copy, Move etc. 

#### Versioning

Our files in Amazon S3 can be versioned but it has to be enabled first at the bucket level. That means that if you re-upload a file version with the same key, it will not be overridden. It will create a new version of that file
like version one, then version two, then version three and so on. It is best practice to version your buckets in Amazon S3 in order to be able to have all the file versions for a while because we can get protected against 
unintended deletes, and are able to restore a previous version, and also you can easily rollback to any previous versions you wanted.
+ Any file that is not versioned prior to enabling versioning will have the version, Null
+ If you suspend versioning in your bucket, it does not delete the previous versions, it will just make sure that the future files do not have a version assigned to it.

#### Versioning Handson

Bucket > Properties > Versioning > Enable Versioning > Save
+ This is to keep multiple versions of an object in the same bucket. Back in Overview, now we have a new panel called Versions and we can hide or show it. If we click show, we have a different UI into Amazon S3 console
  which is showing the version ID on top of the file name. 
+ So for a file uploaded before versioning enabled, the version ID is Null. 
+ Now we add a new file and see that it has a version ID which is a complicated string. If we reupload the file, we see now we have 2 versions of the file each with a different version id. Even though the files were same they were 
  not overwritten. But if we click hide on the versioning tab, we just see 2 files, not the multiple versions. 
+ Now we delete a file and my file is gone. But if we click on show in versioning, we see that a delete marker has been added for the file. A delete marker is a file with zero size that has a version ID and the delete marker 
  is hiding my file since it is deleted. But what I can always restore a previous version. I can delete the delete marker and the latest version is one of the previous versions and my file is back. This is how we can prevent
  against unintended deletes.
+ If we go ahead and delete a specific version of a file, then that version is deleted permanently. 
+ We can suspend the versioning. Any previous objects are still here and still have a version ID. It's just affecting future version objects.

#### Encryption

When we upload objects onto Amazon S3, we upload the objects into servers of AWS. We want to ensure that these objects are not accessible and follow security standards set up by your company.
So Amazon gives you four methods to encrypt objects in Amazon S3.

+ SSE-S3 - This is to encrypt S3 objects using keys handled and managed by AWS.
+ SSE-KMS - This is to leverage AWS key management service to manage your encryption keys.
+ SSE-C - When you manage your own encryption keys 
+ Client Side Encryption

###### SSE-S3

This is an encryption, where the keys used to encrypt the data are handled and managed by Amazon S3. The object is going to be encrypted server side. SSE means Server Side Encryption. And the type of encryption is AES-256,
which is an algorithm. To upload an object and set the SSE-S3 encryption, you must set a header called x-amz-server-side-encryption to AES256. x-amz stands for x Amazon. Amazon S3, thanks to this header, knows that it should apply
its own S3 managed data key. And using the S3 managed data key some encryption will happen and the object will be stored encrypted into your Amazon S3 buckets. The data key is entirely owned and managed by Amazon S3.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SSE_S3.png" width="60%" height="60%"/>

###### SSE-KMS

KMS is Key Management Service, which is an encryption service for you. So SSE-KMS is when you have your encryption keys that are handled and managed by the KMS service. This gives you control over who has
access to what keys and also gives you an audit trail. Each object is going to be again encrypted server side. And for this to work, we must set the header x-amz-server-side-encryption to a value of AWS:KMS.
This is also server side encryption. We use the header to let Amazon S3 know to apply the KMS customer master key you have defined on top of the object. And using this customer master key our objects 
will be encrypted server side and stored in our S3 buckets under the SSE-KMS encryption scheme.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SSE_KMS.png" width="60%" height="60%"/>

###### SSE-C

This stands for server-side encryption. Here we use keys that you provide yourself outside of AWS. Amazon S3 does not store the encryption key you provide. It will use it for encryption and then that key
will be discarded. For this, To transmit the data into AWS, you must use HTTPS, because you're going to send a secret to AWS. And so you must have encryption in transit. Encryption key must be provided
in the HTTP headers for every HTTP request made because it's going to be discarded every single time. So we have the object and we want to have it encrypted in Amazon S3 with our own Client Side data key.
So we send both these things over HTTPS. It's an encrypted connection between the clients and Amazon S3, and the data key is in the header. It is again server side encryption.Amazon S3 will perform the 
encryption and store the encrypted object into your S3 buckets. If you wanted to retrieve that file from Amazon S3 using SSE-C, you would need to provide the same Client Side data key that was used.
So it requires a lot more management on your end, because you manage the the data keys. And Amazon or AWS in general does not know which data keys you have used.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SSE_C.png" width="60%" height="60%"/> 

###### Client Side Encryption.

This is when the client encrypts the objects before uploading it into Amazon S3. Some client libraries like Amazon S3 Encryption Client is a way to perform that Client Side Encryption.
Clients must encrypt data before sending it to S3. And then in case you receive data that is encrypted using Client Side Encryption. Then you are solely responsible for decrypting the data yourself as well.
So you need to make sure you have the right key available. Here the customer manages entirely the keys and the encryption cycle. Amazon S3, this time is just a bucket where it's not doing any encryption for us because it is Client Side Encryption and not Server Side Encryption. The client will use a encryption SDK, like the S3 encryption SDK and the encryption will happen client Side. We are going to just 
upload that already encrypted object into Amazon S3.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/CUSTOMER.png" width="60%" height="60%"/> 

#### Important points

Encryption in transit is around SSL and TLS connections. Amazon S3 is an HTTP service. And it exposes HTTP endpoint that is not encrypted, and it exposes an HTTPS endpoint, which is encrypted and provide what's called encryption in flight. It relies on SSL and TLS certificates. So you're free to use the endpoints you want. But if you use the console, for example, you would be using HTTPS.
Most clients would, use HTTPS endpoints by default. If you're using HTTPS, that means that the data transfer between your clients and Amazon S3 is going to be fully encrypted. And that's what's called encryption in transit.

In case you're using SSE-C, so server side encryption, and the key is provided by your clients, then HTTPS is mandatory. 

#### Encryption handson

If I click on my file right now, and we go on the right hand side, right now it says encryption none. So my file is not encrypted. 
So we upload a file and click Next. On the encryption side, there is two or there are three options. There's None, which is no encryption. And there is Amazon S3 master key(SSE-S3) and Amazon AWS-KMS master key(SSE-KMS). If we choose SSE-S3, We don't have to provide any key. But if we use SSE-KMS, then we have to select a KMS key. And by default, there is going to be a service CMK. So AWS-S3 which is a key
dedicated to Amazon S3 that we could use. Or we could provide our own custom KMS key, in which case, we would have to create that key in advance. We will use Amazon S3 Master key, click on Next. And finally upload. Our file has been uploaded, and on the right hand side, it says encryption AES-256. This has been encrypted. If I go to versions show, we can see the latest version of our file is encrypted, but the version prior to that is not encrypted. So the encryption really belongs to the object you have uploaded. Now I will choose KMS and choose AWS/S3 and click on Upload. and this time the encryption scheme is AWS-KMS. We cannot do SSE-C, through the AWS console, we can do it using the command line interface.

#### Default Encryption

If I go to properties, We have Default Encryption, to automatically encrypt objects stored in Amazon S3. This means that if you are uploading an object, and you don't specify an encryption scheme,
we get three options. Either we leave it unencrypted, or we by default, apply AES-256 or AWS-KMS. So for example, we can say I want to automatically encrypt all my objects with SSE S3 and click on Save.
And now by default if we upload an object without setting any encryption, we can see the encryption is AES-256. 

#### S3 Security and Bucket Policies

+ First we have user-based security. So our IAM users have IAM policies, and they authorize which API calls should be allowed and if our user is authorized through IAM policy how to access our Amazon S3   
  bucket, then it's going to be able to access it.
+ Then we have resource-based security 
  + S3 bucket policies - They're bucket-wide rules that we can set in the S3 console and what they do is that they will say what principals can and cannot do on our S3 bucket. And this enables us to do 
    cross account access to our S3 buckets.
  + Object ACL which is finer grained where we set at the object level the access rule.
  + Bucket ACL, even less common
+ An IAM principal, so it can be a user, a role, can access an S3 object if the IAM permissions allow it, so that means that you have an IAM policy attached to that principal that allows access
  to your S3 bucket, or if the resource policy, usually your S3 bucket policy, allows it.
+ And you need to make sure there is no explicit deny. So if your user through IAM is allowed to access to your S3 bucket but your bucket policy is explicitly denying your user to access it, then you will 
  not be able to access it.

###### S3 bucket policies.

They're JSON-based policies. In the following example, the JSON bucket policy allows public read on our S3 buckets. It says effect allow, principal is ```*```, so anyone, can perform the the action GetObject,
on the resource, ```examplebucket/*``` which means on any objects within my S3 bucket. So this allows public access to our S3 buckets. These bucket policies can be applied to your buckets and objects, so both. The actions is they allow a set of API to allow or deny. The effect is allow or deny, the principal is the account or the user that this S3 bucket policy applies to.

Some common use cases for S3 bucket policies is to grant public access to a bucket or to force objects to be encrypted at the upload time, or to grant access to another account using cross account S3 bucket policies.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/S3_BUCKET_POLICY.png" width="60%" height="60%"/> 

#### BLOCK PUBLIC ACCESS

We also have the bucket settings for block public access. So this was a new setting that was created to block objects from being public if the account had some restrictions.
We have four different kinds of block public access settings.
+ new access control lists
+ any access control lists
+ new public or access point policies. So this is going to block 
  objects and buckets from becoming public if they're granted through any of these methods
+ you can block public and cross account access to buckets and objects 
  through any public bucket or access point policy.
These settings historically were created to prevent company data leaks because there were a lot of leaks of Amazon S3 bucket in the news and Amazon S3 came up with this way of making sure that any server 
could say, hey, none of my buckets are public, by the way, because of the settings, and that was very popular. If you know that your buckets should never, ever be public, leave these on. And there's a way to set these at the account level. 

#### Other Security features of S3
+ On the networking side, you can access S3 privately through VPC endpoints. So if you have EC2 instances in your VPC without internet access, then they can access S3 privately through what's called a VPC
  endpoint.
+ For logging audit, you can use S3 access logs and they can be stored in the other S3 buckets.
+ API calls can also be logged into CloudTrail, which is a service to log API calls in your accounts.
+ For user security, you have MFA Delete, so multifactor authentication is MFA. In which case if you want to delete a specific version of objects in your buckets, then you can enable MFA Delete 
  and we will need to be authenticated with MFA to be able to delete the objects.
+ Pre-signed URLs which was a very, very long URL signed with some credentials from AWS and it's valid only for a limited time. The use case for it, is to download a premium video from a service if the user 
  is logged in and has purchased that video. If you see the access of certain files to certain users for a limited amount of time, think pre-signed URLs.

#### Bucket policies handson

+ S3 bucket policy - Go to "Permissions" - "Bucket Policy." Here we are able to set a bucket policy for our S3 buckets. We can reference Documentation to get some sample policies or use the Policy Generator.
  We're going to design an Amazon S3 bucket policy, that will prevent us from uploading an object unencrypted without AES-256 encryption. So select policy type as "S3 Bucket Policy," and then the "Effect" is going to be "Deny."
  We will deny principal star, which means anyone. For the "Actions," we're talking about upload, so the action is called "Put Objects." So, we deny anyone on Amazon S3, for the action, "Put Objects," and then we have to give 
  the Amazon Resource Name, or ARN. This policy will be applied to anything within the buckets. So, at the end of the ARN, you need to add a ```/*``` which represents any object within the buckets.Then we add the statements.
  Then, we need to add conditions. The first condition is ensuring a header for encryption is not null.The condition is null and key is going to be the encryption header name and value is true. The  second condition is that the header 
  value is AES256 so condition is StringNotEquals and Key is s3:x-amz-server-side-encryption and value is AES256. Thwn we generate a policy. And now if we try to upload a non encypted file it will fail(forbidden). KMSencryption will also
  not work here.  

+ We also have access control lists here where we can set "Access Control List" at the bucket level or on this object under permissions.

+ If we go to "Permissions," - "Block public access," there are four settings which are blocking all public access. To make your bucket public, we need to un-tick everything and click on "Save," These settings can also be applied at the 
  account level. So under account settings we have to enable "Block public access" when we know that all the S3 buckets in your account should not be public. This will provide an extra security.

#### S3 Websites

S3 can host static websites and have them accessible on the worldwide web and the website URLs will be very simple, it will be an HTTP endpoint.
It will look like this.
+ <bucket-name>.s3-website-<AWS-region>.amazonaws.com or <bucket-name>.s3-website.<AWS-region>.amazonaws.com depending on the region you're in.
It starts with a bucket name.s3-website and then the AWS region.amazonaws.com. 
If you enable it for a website but you don't set a bucket policy that allows public access to your bucket, you will get a 403 forbidden error.

We have created index.html and error.html and uploaded them to our bucket. Now we will go to Properties and then Static website hosting and I will say yes, please use this bucket to host a website.
The index document is the document that you get by default which in this case is index.html and the error document is the document that will be showing in case I have the wrong URL, which is error.html.
For redirection rules, we'll just leave it empty and click on Save. After that we will get an endpoint which is the URL of the website hosted on S3. If we click on it we will get a 403 forbidden access denied.
This is because the S3 bucket is a private S3 bucket. So we need to make sure that this S3 bucket becomes public so that we can access our files in here. So we have to do 2 things. 
+ We need to change these block public access settings to make sure that we do enable this bucket as a public bucket.
+ We need to create a bucket policy and we will allow anyone on Amazon S3 to view the website(do a get object). We will give the ARN name/* We generate the policy and click on save. We get a message saying 
  "This bucket has public access,"
Now if we use a wrong URL, it will redirect us to error.html.

```
index.html

<html>
	<head>
		<title>My First Webpage</title>
	</head>
	<body>
		<h1> I love coffee </h1>
		<p> Hello World </>
	</body>

	<img src="coffee.jpg" width=500/>	
</html>

error.html
<h1>Uh oh, there was an error</h1>
```

#### S3 Cors

CORS is Cross-Origin Resource Sharing.
+ Origin - An origin is a scheme, so a protocol, a host, a domain, and a port. In English, this means that if we go to https://www.example.com, this is an origin where the scheme is HTTPS, the host is www.example.com, 
  and the port is port 443.
Cross-Origin Resource Sharing means that we want to get resources from a different origin. The Web Browser has CORS security in place which means as soon as you visit a website, you can make request to other origins 
only if the other origins allow you to make these request. This is a browser-based security. So what is the same origin and what is a different origin? 
+ Same origin is where you go on example.com/app1 or example.com/app2. This is the same origin, so we can make requests from the web browser from the first URL to the second URL because this is the same origin.
+ But if you visit, for example, www.example.com and then you're asking your web browser to make a request to other.example.com, this is what's called a cross-origin request and your web browser will block it
  unless you have the correct CORS headers. The request will not be fulfilled unless the other origin allows for the request using the CORS Headers. The CORS Headers is called Access-Control-Allow-Origin.

Our web browser visits the first web server, and because this is the first visit we do, it's called the origin. So for example, our web server is at https://www.example.com. There is a second web server called a 
cross-origin because it has a different url, which is https://www.other.com. So the web browser visits our first origin and it's going to be asked from the files(index.html) that are uploaded at the origin to make a 
request to the cross-origin. The web browser will do a preflight request which will ask the cross-origin if it is allowed to do a request on it. The origin reponds with headers like Access-Control-Allow-Origin which states if
this website is allowed or not. If it returns methods that are authorized as GET, PUT, and DELETE. we can get a file, delete a file, or update the file.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/CORS.png" width="60%" height="60%"/> 

In S3 CORS, if a client does a cross-origin request, on our S3 bucket enabled as a website, then we need to enable the right CORS headers. We can allow for a specific origin by specifying the entire origin name, or as star * 
for all origins. The web browser for example, is getting HTML files from our bucket enabled as a website. But there is second bucket that is going to be our cross-origin bucket, also enabled as a website, that contains some files 
that we want. So, we're going to do GET index.html and the website will say, okay here is your index.html and that file is going to say you need to perform a GET for another file on the other origin. And if the other bucket is configured
with the right CORS headers, then web browser will be able to make the request, if not it will not be able to make that request. The CORS headers have to be defined on the cross-origin bucket, not the first origin bucket.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/CORS_S3.png" width="60%" height="60%"/> 

```
index.html

<html>
	<head>
		<title>My First Webpage</title>
	</head>
	<body>
		<h1> I love coffee </h1>
		<p> Hello World </>
	</body>

	<img src="coffee.jpg" width=500/>	

	<div id="tofetch"/>
	<script>
		var tofetch = document.getElementById("tofetch");
		fetch('use the cors bucket/extra-page.html').then((response) => {
					return response.text();
				}).then((html) => {
					tofetch.innerHTML = html
				});
	</script>
</html>

extra-page.html

<p>This <strong>extra page</strong> has been successfully loaded!</p>
```

We upload index.html and error.html to the primary bucket and extra-page.html to the cross origin bucket. Then we make both buckets public and enable static page hosting. Then we add CORS config to the CORS bucket and everything works.
```
CORS_CONFIG.xml
<CORSConfiguration>
    <CORSRule>
        <AllowedOrigin>CORS bucket url here, we can also put * </AllowedOrigin>
        <AllowedMethod>GET</AllowedMethod>
        <AllowedMethod>POST</AllowedMethod>
        <AllowedMethod>DELETE</AllowedMethod>
        <MaxAgeSeconds>3000</MaxAgeSeconds>
        <AllowedHeader>Authorization</AllowedHeader>
    </CORSRule>   
</CORSConfiguration>
```
#### S3 Consistency model

Amazon S3 is a new eventually consistent system. It is made up of multiple servers so when you write to Amazon S3, the other servers are going to replicate data between each other. And this is what leads to different consistency issues.
+ You get read after write consistency for PUTS of new objects so that means that as soon as you upload a new object, once you get a correct response from Amazon S3, then you can do a GET of that object and get it. So that means 
  that if you do a successful PUTS so PUT 200, Then you can do a GET and it will be 200. 
+ If you do a GET before doing the PUT to check if the object existed, you may get a 404 for not existing. There is a chance that you would get a 404 still even though the object was already uploaded and this is what's called eventually  
  consistent. So we have eventual consistency, on DELETES and PUTS of existing objects. So if you read an object right after updating it, you may get the older version of that object. If you do a PUT on an existing object so PUT 200, then you do another PUT 200, and then you do a GET, then the GET might return the older version if you are very, very quick. To get the newer version, you have to just to wait a little bit.
+ If you delete an object, you might still be able to retrieve it for a very short time. So if you do delete an object and you do a GET right after , you may have a successful GET, so GET 200. If you retry after a second or five seconds,
  then the GET will give you a 404 because the object has been deleted.
So read after write consistency for PUTS of new objects and eventual consistency for DELETES and PUTS of existing objects. There is no way to request strong consistency in Amazon S3, you only get eventual consistency and there's no API to  get strong consistency. So that means that if you overwrite an object, you need to wait a little bit before you are certain that the GET returns the newest version of your object.