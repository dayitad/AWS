## S3

Amazon S3 powers the biggest websites in the world and it is one of the most important building blocks of AWS. It is advertised as an infinitely scaling storage so we don't need to provision its size in advance,
and it can grow infinitely. It's extremely popular and we can deploy websites on Amazon S3. Many AWS resources have an integration with Amazon S3 as well.

#### Buckets and objects

+ S3 is a system, a service that allows us to store objects, so files, into buckets, or directories,
+ Each bucket must have a globally unique name. We can't create a bucket that has already been taken in terms of names. The buckets are defined at the region level. So even though S3 is a global service, buckets are a 
  regional resource,
+ There is a naming convention for buckets which include no uppercase, no underscore, three to 63 characters long. It should not be an IP, and it must start with a lowercase letter or a number.
+ In these S3 buckets we need to create objects, and objects are files and they must have a key.
  + A key is the full path to that file. 
  	+ So if we have a bucket named my-bucket, and a object named my_file.txt, then the key is my_file.txt in s3://my-bucket/my_file.txt.
  	+ But if we have folder structures within our S3 bucket like my_folder1/another_folder/my_file.txt, the key is my_folder1/another_folder/my_file.txt in s3://my-bucket/my_folder1/another_folder/my_file.txt
  + The key can be decomposed in two things, the key prefix and the object name.
  	+ The prefix is my_folder1/another_folder/ and the object name is my_file.txt.
+ There's no concept of directories within buckets, just very, very long key names, but the UI will try to trick you into thinking otherwise because we could "create" directories within S3. But what we really have in S3 is just keys
  with very long names that contain slashes.
+ The object values are the content of the body.
  + A maximum object size on Amazon S3 is 5 TB, so 5,000 gigabytes, which is a huge object but we cannot upload more than 5 GB at a time. 
    That means that if you want to upload a big object of five terabytes, you must divide that object into parts of less than 5 GB, and upload these parts independently into what's called a multi-part upload.
+ Each object in Amazon S3 can have metadata which are key value pairs that could be system or user metadata, and this is to add information onto your objects.
+ Tags - Unicode key value pairs up to 10 which is very useful when you want to have security on your objects, or lifecycle policies.
+ Finally, we'll see when we go into versioning that there is a version ID onto our Amazon S3 bucket.

#### Handson

Go to Amazon S3 console. S3 does not require region selection which makes S3 a global service. But all our buckets are created for a specific region. When we create a bucket, we will still have to select a region.
This Global view will give us all the buckets across all the regions, but a bucket is tied to a specific region. So let us create a bucket. We enter a bucket name and the name must be globally unique. If the bucket name
exists in someone else's account, it's already taken. So lets select a unique bucket name. Then we have to select a region but we should select a region that's close to you as well to create your S3 buckets, and as you can see
not all the regions have S3 just yet. This bucket is going to be tied to this region, even though it will show up as a global service. In bucket settings, there is Block Public Access to prevent your bucket from being public 
by mistake and keep our bucket as private to prevent data leaks. The we click on Create bucket. The bucket has been created, and now I can Go to the bucket details, either by clicking the link, or by clicking the link in the 
console as well. Now we can upload a file. Upload > Add file. Then we get options like information about the permissions of who can see that file, properties on how we want to store that object or Storage class, Encryption, metadata
and tags. > Click on Upload. Now my file is being uploaded onto my S3 bucket. I can click on it, and there's a panel on the right hand side that comes up, and it gives us some information about our file. I'm going to right click on 
this file, and then click on Open. So through this way, I am able to view my file. If I click on the Object URL, I get access denied. So using this public URL for my file in S3, I get a 403 access denied because this file is not public.
But if we use the right click and Open it actually creates a special URL, and this is a very special URL that's very, very long, and this URL is called the pre-signed URL. It's signed with my credentials on AWS, and so because I have the power to view this file, it's going to be included in that URL and I'm going to be able to access this file. We can create a folder called images, and click on Save, and then within this folder, I can go ahead again to upload my file.
If we look at our S3 bucket, we have two keys. We have one file at the root of our S3 bucket and another within the key images. This gives us the illusion of a directory. We can perform operations like rename the file, or delete the file, 
Copy, Move etc. 

#### Versioning

Our files in Amazon S3 can be versioned but it has to be enabled first at the bucket level. That means that if you re-upload a file version with the same key, it will not be overridden. It will create a new version of that file
like version one, then version two, then version three and so on. It is best practice to version your buckets in Amazon S3 in order to be able to have all the file versions for a while because we can get protected against 
unintended deletes, and are able to restore a previous version, and also you can easily rollback to any previous versions you wanted.
+ Any file that is not versioned prior to enabling versioning will have the version, Null
+ If you suspend versioning in your bucket, it does not delete the previous versions, it will just make sure that the future files do not have a version assigned to it.

#### Versioning Handson

Bucket > Properties > Versioning > Enable Versioning > Save
+ This is to keep multiple versions of an object in the same bucket. Back in Overview, now we have a new panel called Versions and we can hide or show it. If we click show, we have a different UI into Amazon S3 console
  which is showing the version ID on top of the file name. 
+ So for a file uploaded before versioning enabled, the version ID is Null. 
+ Now we add a new file and see that it has a version ID which is a complicated string. If we reupload the file, we see now we have 2 versions of the file each with a different version id. Even though the files were same they were 
  not overwritten. But if we click hide on the versioning tab, we just see 2 files, not the multiple versions. 
+ Now we delete a file and my file is gone. But if we click on show in versioning, we see that a delete marker has been added for the file. A delete marker is a file with zero size that has a version ID and the delete marker 
  is hiding my file since it is deleted. But what I can always restore a previous version. I can delete the delete marker and the latest version is one of the previous versions and my file is back. This is how we can prevent
  against unintended deletes.
+ If we go ahead and delete a specific version of a file, then that version is deleted permanently. 
+ We can suspend the versioning. Any previous objects are still here and still have a version ID. It's just affecting future version objects.

#### Encryption

When we upload objects onto Amazon S3, we upload the objects into servers of AWS. We want to ensure that these objects are not accessible and follow security standards set up by your company.
So Amazon gives you four methods to encrypt objects in Amazon S3.

+ SSE-S3 - This is to encrypt S3 objects using keys handled and managed by AWS.
+ SSE-KMS - This is to leverage AWS key management service to manage your encryption keys.
+ SSE-C - When you manage your own encryption keys 
+ Client Side Encryption

###### SSE-S3

This is an encryption, where the keys used to encrypt the data are handled and managed by Amazon S3. The object is going to be encrypted server side. SSE means Server Side Encryption. And the type of encryption is AES-256,
which is an algorithm. To upload an object and set the SSE-S3 encryption, you must set a header called x-amz-server-side-encryption to AES256. x-amz stands for x Amazon. Amazon S3, thanks to this header, knows that it should apply
its own S3 managed data key. And using the S3 managed data key some encryption will happen and the object will be stored encrypted into your Amazon S3 buckets. The data key is entirely owned and managed by Amazon S3.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SSE_S3.png" width="60%" height="60%"/>

###### SSE-KMS

KMS is Key Management Service, which is an encryption service for you. So SSE-KMS is when you have your encryption keys that are handled and managed by the KMS service. This gives you control over who has
access to what keys and also gives you an audit trail. Each object is going to be again encrypted server side. And for this to work, we must set the header x-amz-server-side-encryption to a value of AWS:KMS.
This is also server side encryption. We use the header to let Amazon S3 know to apply the KMS customer master key you have defined on top of the object. And using this customer master key our objects 
will be encrypted server side and stored in our S3 buckets under the SSE-KMS encryption scheme.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SSE_KMS.png" width="60%" height="60%"/>

###### SSE-C

This stands for server-side encryption. Here we use keys that you provide yourself outside of AWS. Amazon S3 does not store the encryption key you provide. It will use it for encryption and then that key
will be discarded. For this, To transmit the data into AWS, you must use HTTPS, because you're going to send a secret to AWS. And so you must have encryption in transit. Encryption key must be provided
in the HTTP headers for every HTTP request made because it's going to be discarded every single time. So we have the object and we want to have it encrypted in Amazon S3 with our own Client Side data key.
So we send both these things over HTTPS. It's an encrypted connection between the clients and Amazon S3, and the data key is in the header. It is again server side encryption.Amazon S3 will perform the 
encryption and store the encrypted object into your S3 buckets. If you wanted to retrieve that file from Amazon S3 using SSE-C, you would need to provide the same Client Side data key that was used.
So it requires a lot more management on your end, because you manage the the data keys. And Amazon or AWS in general does not know which data keys you have used.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/SSE_C.png" width="60%" height="60%"/> 

###### Client Side Encryption.

This is when the client encrypts the objects before uploading it into Amazon S3. Some client libraries like Amazon S3 Encryption Client is a way to perform that Client Side Encryption.
Clients must encrypt data before sending it to S3. And then in case you receive data that is encrypted using Client Side Encryption. Then you are solely responsible for decrypting the data yourself as well.
So you need to make sure you have the right key available. Here the customer manages entirely the keys and the encryption cycle. Amazon S3, this time is just a bucket where it's not doing any encryption for us because it is Client Side Encryption and not Server Side Encryption. The client will use a encryption SDK, like the S3 encryption SDK and the encryption will happen client Side. We are going to just 
upload that already encrypted object into Amazon S3.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/CUSTOMER.png" width="60%" height="60%"/> 

#### Important points

Encryption in transit is around SSL and TLS connections. Amazon S3 is an HTTP service. And it exposes HTTP endpoint that is not encrypted, and it exposes an HTTPS endpoint, which is encrypted and provide what's called encryption in flight. It relies on SSL and TLS certificates. So you're free to use the endpoints you want. But if you use the console, for example, you would be using HTTPS.
Most clients would, use HTTPS endpoints by default. If you're using HTTPS, that means that the data transfer between your clients and Amazon S3 is going to be fully encrypted. And that's what's called encryption in transit.

In case you're using SSE-C, so server side encryption, and the key is provided by your clients, then HTTPS is mandatory. 

#### Encryption handson

If I click on my file right now, and we go on the right hand side, right now it says encryption none. So my file is not encrypted. 
So we upload a file and click Next. On the encryption side, there is two or there are three options. There's None, which is no encryption. And there is Amazon S3 master key(SSE-S3) and Amazon AWS-KMS master key(SSE-KMS). If we choose SSE-S3, We don't have to provide any key. But if we use SSE-KMS, then we have to select a KMS key. And by default, there is going to be a service CMK. So AWS-S3 which is a key
dedicated to Amazon S3 that we could use. Or we could provide our own custom KMS key, in which case, we would have to create that key in advance. We will use Amazon S3 Master key, click on Next. And finally upload. Our file has been uploaded, and on the right hand side, it says encryption AES-256. This has been encrypted. If I go to versions show, we can see the latest version of our file is encrypted, but the version prior to that is not encrypted. So the encryption really belongs to the object you have uploaded. Now I will choose KMS and choose AWS/S3 and click on Upload. and this time the encryption scheme is AWS-KMS. We cannot do SSE-C, through the AWS console, we can do it using the command line interface.

#### Default Encryption

If I go to properties, We have Default Encryption, to automatically encrypt objects stored in Amazon S3. This means that if you are uploading an object, and you don't specify an encryption scheme,
we get three options. Either we leave it unencrypted, or we by default, apply AES-256 or AWS-KMS. So for example, we can say I want to automatically encrypt all my objects with SSE S3 and click on Save.
And now by default if we upload an object without setting any encryption, we can see the encryption is AES-256. 

#### S3 Security and Bucket Policies

+ First we have user-based security. So our IAM users have IAM policies, and they authorize which API calls should be allowed and if our user is authorized through IAM policy how to access our Amazon S3   
  bucket, then it's going to be able to access it.
+ Then we have resource-based security 
  + S3 bucket policies - They're bucket-wide rules that we can set in the S3 console and what they do is that they will say what principals can and cannot do on our S3 bucket. And this enables us to do 
    cross account access to our S3 buckets.
  + Object ACL which is finer grained where we set at the object level the access rule.
  + Bucket ACL, even less common
+ An IAM principal, so it can be a user, a role, can access an S3 object if the IAM permissions allow it, so that means that you have an IAM policy attached to that principal that allows access
  to your S3 bucket, or if the resource policy, usually your S3 bucket policy, allows it.
+ And you need to make sure there is no explicit deny. So if your user through IAM is allowed to access to your S3 bucket but your bucket policy is explicitly denying your user to access it, then you will 
  not be able to access it.

###### S3 bucket policies.

They're JSON-based policies. In the following example, the JSON bucket policy allows public read on our S3 buckets. It says effect allow, principal is ```*```, so anyone, can perform the the action GetObject,
on the resource, ```examplebucket/*``` which means on any objects within my S3 bucket. So this allows public access to our S3 buckets. These bucket policies can be applied to your buckets and objects, so both. The actions is they allow a set of API to allow or deny. The effect is allow or deny, the principal is the account or the user that this S3 bucket policy applies to.

Some common use cases for S3 bucket policies is to grant public access to a bucket or to force objects to be encrypted at the upload time, or to grant access to another account using cross account S3 bucket policies.

<img src="https://raw.githubusercontent.com/dhrub123/AWS/master/S3/images/S3_BUCKET_POLICY.png" width="60%" height="60%"/> 

#### BLOCK PUBLIC ACCESS

We also have the bucket settings for block public access. So this was a new setting that was created to block objects from being public if the account had some restrictions.
We have four different kinds of block public access settings.
+ new access control lists
+ any access control lists
+ new public or access point policies. So this is going to block 
  objects and buckets from becoming public if they're granted through any of these methods
+ you can block public and cross account access to buckets and objects 
  through any public bucket or access point policy.
These settings historically were created to prevent company data leaks because there were a lot of leaks of Amazon S3 bucket in the news and Amazon S3 came up with this way of making sure that any server 
could say, hey, none of my buckets are public, by the way, because of the settings, and that was very popular. If you know that your buckets should never, ever be public, leave these on. And there's a way to set these at the account level. 

#### Other Security features of S3
+ On the networking side, you can access S3 privately through VPC endpoints. So if you have EC2 instances in your VPC without internet access, then they can access S3 privately through what's called a VPC
  endpoint.
+ For logging audit, you can use S3 access logs and they can be stored in the other S3 buckets.
+ API calls can also be logged into CloudTrail, which is a service to log API calls in your accounts.
+ For user security, you have MFA Delete, so multifactor authentication is MFA. In which case if you want to delete a specific version of objects in your buckets, then you can enable MFA Delete 
  and we will need to be authenticated with MFA to be able to delete the objects.
+ Pre-signed URLs which was a very, very long URL signed with some credentials from AWS and it's valid only for a limited time. The use case for it, is to download a premium video from a service if the user 
  is logged in and has purchased that video. If you see the access of certain files to certain users for a limited amount of time, think pre-signed URLs.

#### S3 Websites

#### S3 Cors

#### S3 Consistency model

